{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12a7ba78-57fa-47d4-9eed-b141a5d2e8f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: yfinance in c:\\users\\msofy\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.2.38)\n",
      "Requirement already satisfied: pandas>=1.3.0 in c:\\users\\msofy\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from yfinance) (2.0.3)\n",
      "Requirement already satisfied: numpy>=1.16.5 in c:\\users\\msofy\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from yfinance) (1.26.4)\n",
      "Requirement already satisfied: requests>=2.31 in c:\\users\\msofy\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from yfinance) (2.31.0)\n",
      "Requirement already satisfied: multitasking>=0.0.7 in c:\\users\\msofy\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from yfinance) (0.0.11)\n",
      "Requirement already satisfied: lxml>=4.9.1 in c:\\users\\msofy\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from yfinance) (5.2.1)\n",
      "Requirement already satisfied: appdirs>=1.4.4 in c:\\users\\msofy\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from yfinance) (1.4.4)\n",
      "Requirement already satisfied: pytz>=2022.5 in c:\\users\\msofy\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from yfinance) (2024.1)\n",
      "Requirement already satisfied: frozendict>=2.3.4 in c:\\users\\msofy\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from yfinance) (2.4.2)\n",
      "Requirement already satisfied: peewee>=3.16.2 in c:\\users\\msofy\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from yfinance) (3.17.3)\n",
      "Requirement already satisfied: beautifulsoup4>=4.11.1 in c:\\users\\msofy\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from yfinance) (4.12.3)\n",
      "Requirement already satisfied: html5lib>=1.1 in c:\\users\\msofy\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from yfinance) (1.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\msofy\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from beautifulsoup4>=4.11.1->yfinance) (2.5)\n",
      "Requirement already satisfied: six>=1.9 in c:\\users\\msofy\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from html5lib>=1.1->yfinance) (1.16.0)\n",
      "Requirement already satisfied: webencodings in c:\\users\\msofy\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from html5lib>=1.1->yfinance) (0.5.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\msofy\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas>=1.3.0->yfinance) (2.9.0.post0)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\msofy\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas>=1.3.0->yfinance) (2024.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\msofy\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.31->yfinance) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\msofy\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.31->yfinance) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\msofy\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.31->yfinance) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\msofy\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.31->yfinance) (2024.2.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install yfinance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "db1a74f9-93b2-433b-af72-393fdde154f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTS\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#Fin Data Sources\n",
    "import yfinance as yf\n",
    "import pandas_datareader as pdr\n",
    "\n",
    "#Data viz\n",
    "import plotly.graph_objs as go\n",
    "import plotly.express as px\n",
    "\n",
    "import time\n",
    "from datetime import date\n",
    "\n",
    "# for graphs\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c3650f5-8394-4fbe-b92d-a0866da361b2",
   "metadata": {},
   "source": [
    "Question 1: IPO Filings Web Scraping and Data Processing\n",
    "\n",
    "What's the total sum ($m) of 2023 filings that happened on Fridays?\n",
    "\n",
    "Re-use the [Code Snippet 1] example to get the data from web for this endpoint: https://stockanalysis.com/ipos/filings/ Convert the 'Filing Date' to datetime(), 'Shares Offered' to float64 (if '-' is encountered, populate with NaNs). Define a new field 'Avg_price' based on the \"Price Range\", which equals to NaN if no price is specified, to the price (if only one number is provided), or to the average of 2 prices (if a range is given). You may be inspired by the function extract_numbers() in [Code Snippet 4], or you can write your own function to \"parse\" a string. Define a column \"Shares_offered_value\", which equals to \"Shares Offered\" * \"Avg_price\" (when both columns are defined; otherwise, it's NaN)\n",
    "\n",
    "Find the total sum in $m (millions of USD, closest INTEGER number) for all filings during 2023, which happened on Fridays (Date.dt.dayofweek()==4). You should see 32 records in total, 25 of it is not null.\n",
    "\n",
    "(additional: you can read about S-1 IPO filing to understand the context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0654dd42-d46b-4418-9f50-b96a06720a2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sum in $m for filings on Fridays in 2023: 286\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "# Function to extract numbers from a string\n",
    "def extract_numbers(text):\n",
    "    numbers = re.findall(r'\\d+\\.?\\d*', text)\n",
    "    if len(numbers) == 1:\n",
    "        return float(numbers[0])\n",
    "    elif len(numbers) == 2:\n",
    "        return (float(numbers[0]) + float(numbers[1])) / 2\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Fetching data from the endpoint\n",
    "url = \"https://stockanalysis.com/ipos/filings/\"\n",
    "response = requests.get(url, headers=headers)\n",
    "ipo = pd.read_html(response.text)\n",
    "ipo = ipo_dfs[0]\n",
    "\n",
    "# Convert 'Filing Date' to datetime\n",
    "ipo['Filing Date'] = pd.to_datetime(ipo['Filing Date'])\n",
    "\n",
    "# Convert 'Shares Offered' to float64\n",
    "ipo['Shares Offered'] = ipo['Shares Offered'].replace('-', float('nan')).astype(float)\n",
    "\n",
    "# Define 'Avg_price' based on 'Price Range'\n",
    "ipo['Avg_price'] = ipo['Price Range'].apply(extract_numbers)\n",
    "\n",
    "# Define 'Shares_offered_value'\n",
    "ipo['Shares_offered_value'] = ipo['Shares Offered'] * ipo['Avg_price']\n",
    "\n",
    "# Filter data for filings that occurred on Fridays during 2023\n",
    "friday_filings_2023 = ipo[(ipo['Filing Date'].dt.year == 2023) & (ipo['Filing Date'].dt.dayofweek == 4)]\n",
    "\n",
    "# Calculate total sum in $m\n",
    "total_sum_millions_usd = int(round(friday_filings_2023['Shares_offered_value'].sum() / 1e6))\n",
    "\n",
    "print(\"Total sum in $m for filings on Fridays in 2023:\", total_sum_millions_usd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d44811af-1ba7-4c2f-b934-d6c4ad34c4c4",
   "metadata": {},
   "source": [
    "Question 2: IPOs \"Fixed days hold\" strategy\n",
    "\n",
    "Find the optimal number of days X (between 1 and 30), where 75% quantile growth is the highest?\n",
    "\n",
    "Reuse [Code Snippet 1] to retrieve the list of IPOs from 2023 and 2024 (from URLs: https://stockanalysis.com/ipos/2023/ and https://stockanalysis.com/ipos/2024/). Get all OHLCV daily prices for all stocks with an \"IPO date\" before March 1, 2024 (\"< 2024-03-01\") - 184 tickers (without 'RYZB'). Please remove 'RYZB', as it is no longer available on Yahoo Finance.\n",
    "\n",
    "Sometimes you may need to adjust the symbol name (e.g., 'IBAC' on stockanalysis.com -> 'IBACU' on Yahoo Finance) to locate OHLCV prices for all stocks. Some of the tickers like 'DYCQ' and 'LEGT' were on the market less than 30 days (11 and 21 days, respectively). Let's leave them in the dataset; it just means that you couldn't hold them for more days than they were listed.\n",
    "\n",
    "Let's assume you managed to buy a new stock (listed on IPO) on the first day at the [Adj Close] price]. Your strategy is to hold for exactly X full days (where X is between 1 and 30) and sell at the \"Adj. Close\" price in X days (e.g., if X=1, you sell on the next day). Find X, when the 75% quantile growth (among 185 investments) is the highest.\n",
    "\n",
    "HINTs:\n",
    "\n",
    "    You can generate 30 additional columns: growth_future_1d ... growth_future_30d, join that with the table of min_dates (first day when each stock has data on Yahoo Finance), and perform vector operations on the resulting dataset.\n",
    "    You can use the DataFrame.describe() function to get mean, min, max, 25-50-75% quantiles.\n",
    "\n",
    "Additional:\n",
    "\n",
    "    You can also ensure that the mean and 50th percentile (median) investment returns are negative for most X values, implying a wager for a \"lucky\" investor who might be in the top 25%.\n",
    "    What's your recommendation: Do you suggest pursuing this strategy for an optimal X?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "0915e95d-1f9c-4314-ae05-2edb796d44c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  184 of 184 completed\n",
      "\n",
      "4 Failed downloads:\n",
      "['JVSA', 'DYCQ', 'LEGT']: Exception(\"%ticker%: Data doesn't exist for startDate = 1672549200, endDate = 1709269200\")\n",
      "['PTHR']: Exception('%ticker%: No timezone found, symbol may be delisted')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ticker      AACT  AESI  AFJK  AHR  AITR  AIXI  ALCY  ANL  ANRO  ANSC  ...  \\\n",
      "Date                                                                  ...   \n",
      "2023-01-03   NaN   NaN   NaN  NaN   NaN   NaN   NaN  NaN   NaN   NaN  ...   \n",
      "2023-01-04   NaN   NaN   NaN  NaN   NaN   NaN   NaN  NaN   NaN   NaN  ...   \n",
      "2023-01-05   NaN   NaN   NaN  NaN   NaN   NaN   NaN  NaN   NaN   NaN  ...   \n",
      "2023-01-06   NaN   NaN   NaN  NaN   NaN   NaN   NaN  NaN   NaN   NaN  ...   \n",
      "2023-01-09   NaN   NaN   NaN  NaN   NaN   NaN   NaN  NaN   NaN   NaN  ...   \n",
      "\n",
      "Ticker      VHAI  VSME  VTMX  WBUY  WLGS  WRNT  YGFGF  YIBO  ZJYL  ZKH  \n",
      "Date                                                                    \n",
      "2023-01-03   NaN   NaN   NaN   NaN   NaN   NaN    NaN   NaN   NaN  NaN  \n",
      "2023-01-04   NaN   NaN   NaN   NaN   NaN   NaN    NaN   NaN   NaN  NaN  \n",
      "2023-01-05   NaN   NaN   NaN   NaN   NaN   NaN    NaN   NaN   NaN  NaN  \n",
      "2023-01-06   NaN   NaN   NaN   NaN   NaN   NaN    NaN   NaN   NaN  NaN  \n",
      "2023-01-09   NaN   NaN   NaN   NaN   NaN   NaN    NaN   NaN   NaN  NaN  \n",
      "\n",
      "[5 rows x 184 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import yfinance as yf\n",
    "\n",
    "# Function to retrieve IPO data from stockanalysis.com\n",
    "def get_ipo_data(url):\n",
    "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "    response = requests.get(url, headers=headers)\n",
    "    return pd.read_html(response.text)[0]\n",
    "\n",
    "# Retrieve IPO data for 2023 and 2024\n",
    "url_2023 = \"https://stockanalysis.com/ipos/2023/\"\n",
    "url_2024 = \"https://stockanalysis.com/ipos/2024/\"\n",
    "\n",
    "ipo_2023 = get_ipo_data(url_2023)\n",
    "ipo_2024 = get_ipo_data(url_2024)\n",
    "\n",
    "# Combine IPO data\n",
    "ipo_data = pd.concat([ipo_2023, ipo_2024])\n",
    "\n",
    "# Filter IPOs before March 1, 2024\n",
    "ipo_data['IPO Date'] = pd.to_datetime(ipo_data['IPO Date'])\n",
    "ipo_data = ipo_data[ipo_data['IPO Date'] < '2024-03-01']\n",
    "\n",
    "# Get tickers (adjusting symbol names if necessary)\n",
    "tickers = ipo_data['Symbol'].tolist()\n",
    "tickers.remove('RYZB')  # Remove 'RYZB' as it is no longer available on Yahoo Finance\n",
    "\n",
    "# Get OHLCV daily prices for all tickers\n",
    "start_date = '2023-01-01'\n",
    "end_date = '2024-03-01'\n",
    "prices = yf.download(tickers, start=start_date, end=end_date)['Adj Close']\n",
    "\n",
    "# Display the first few rows of the prices DataFrame\n",
    "print(prices.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "1e8580f4-ea68-46b9-bbe9-8d3ccc72f649",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "\n",
      "1 Failed download:\n",
      "['DYCQ']: Exception(\"%ticker%: Data doesn't exist for startDate = 1672549200, endDate = 1709269200\")\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "\n",
      "1 Failed download:\n",
      "['LEGT']: Exception(\"%ticker%: Data doesn't exist for startDate = 1672549200, endDate = 1709269200\")\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "\n",
      "1 Failed download:\n",
      "['JVSA']: Exception(\"%ticker%: Data doesn't exist for startDate = 1672549200, endDate = 1709269200\")\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The optimal holding period is 28 days.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "from requests import get  # Import get function directly from requests\n",
    "\n",
    "def get_and_analyze_ipos(start_year, end_year):\n",
    "  \"\"\"\n",
    "  Fetches IPO data for a given year range, retrieves prices, calculates growth rates, \n",
    "  and finds the optimal holding period.\n",
    "\n",
    "  Args:\n",
    "      start_year (int): Starting year for IPO data retrieval.\n",
    "      end_year (int): Ending year for IPO data retrieval.\n",
    "\n",
    "  Returns:\n",
    "      int: The optimal holding period (number of days) with the highest 75th percentile growth.\n",
    "  \"\"\"\n",
    "  # Fetch and combine IPO data\n",
    "  ipos = pd.DataFrame()\n",
    "  for year in range(start_year, end_year + 1):\n",
    "    url = f\"https://stockanalysis.com/ipos/{year}/\"\n",
    "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "    response = get(url, headers=headers)\n",
    "    ipos = pd.concat([ipos, pd.read_html(response.text)[0]], ignore_index=True)\n",
    "  ipos['IPO Date'] = pd.to_datetime(ipos['IPO Date'])\n",
    "\n",
    "  # Filter IPOs and define adjustments (optional)\n",
    "  filtered_ipo = ipos[(ipos['IPO Date'] < '2024-03-01') & (ipos['Symbol'] != 'RYZB')]\n",
    "  ticker_adjustments = {\"PTHR\": \"PTHRF\", \"IBAC\": \"IBACU\"}  # Optional adjustments\n",
    "\n",
    "  # Download prices and calculate growth rates\n",
    "  price_data = {}\n",
    "  for symbol in filtered_ipo['Symbol']:\n",
    "    adjusted_symbol = ticker_adjustments.get(symbol, symbol)\n",
    "    try:\n",
    "      df = yf.download(adjusted_symbol, start=\"2023-01-01\", end=\"2024-03-01\")\n",
    "      if df is not None:\n",
    "        for i in range(1, 31):\n",
    "          df[f'growth_{i}d'] = ((df['Adj Close'].shift(-i) / df['Adj Close'] - 1) * 100).round(2)\n",
    "        price_data[symbol] = df\n",
    "    except Exception as e:\n",
    "      print(f\"Error downloading {symbol}: {e}\")\n",
    "\n",
    " # Combine price data (if any) and calculate growth percentiles\n",
    "  results = pd.DataFrame()\n",
    "  for symbol, df in price_data.items():\n",
    "    results = pd.concat([results, df[[f'growth_{i}d' for i in range(1, 31)]]])\n",
    "  growth_quantile = results.quantile(0.75)\n",
    "\n",
    "  # Find optimal holding period with highest 75th percentile growth\n",
    "  optimal_days = growth_quantile.idxmax()\n",
    "\n",
    "  # Analyze additional statistics (mean, median)\n",
    "  growth_describe = results.describe(percentiles=[.5])  # Calculate mean, median\n",
    "\n",
    "# Run the analysis and print the optimal holding period\n",
    "optimal_period = get_and_analyze_ipos(2023, 2024)\n",
    "print(f\"The optimal holding period is {optimal_days} days.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "d40c7acc-867a-4919-ae56-5bc31e90d305",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "\n",
      "1 Failed download:\n",
      "['DYCQ']: Exception(\"%ticker%: Data doesn't exist for startDate = 1672549200, endDate = 1709269200\")\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "\n",
      "1 Failed download:\n",
      "['LEGT']: Exception(\"%ticker%: Data doesn't exist for startDate = 1672549200, endDate = 1709269200\")\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "\n",
      "1 Failed download:\n",
      "['JVSA']: Exception(\"%ticker%: Data doesn't exist for startDate = 1672549200, endDate = 1709269200\")\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The optimal holding period is growth_28d days.\n",
      "Growth statistics (across all investments for each holding period):\n",
      "          growth_1d     growth_2d     growth_3d     growth_4d     growth_5d  \\\n",
      "count  25773.000000  25592.000000  25411.000000  25231.000000  25052.000000   \n",
      "mean      -0.019909      0.009616      0.058067      0.096509      0.152900   \n",
      "std       10.744505     15.177685     18.599278     21.301001     24.182108   \n",
      "min      -88.990000    -93.550000    -93.790000    -93.630000    -93.550000   \n",
      "50%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
      "max      582.690000    597.130000    662.560000    765.050000    795.260000   \n",
      "\n",
      "          growth_6d     growth_7d     growth_8d     growth_9d    growth_10d  \\\n",
      "count  24873.000000  24694.000000  24515.000000  24336.000000  24159.000000   \n",
      "mean       0.223766      0.260309      0.321074      0.401132      0.471337   \n",
      "std       27.032403     28.971771     30.999573     33.078063     34.979911   \n",
      "min      -93.700000    -94.100000    -94.320000    -94.410000    -94.610000   \n",
      "50%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
      "max      930.910000    918.350000    938.060000    846.470000    810.530000   \n",
      "\n",
      "       ...    growth_21d    growth_22d    growth_23d    growth_24d  \\\n",
      "count  ...  22274.000000  22109.000000  21944.000000  21779.000000   \n",
      "mean   ...      0.269019      0.223580      0.159182      0.104095   \n",
      "std    ...     48.156399     49.415266     50.520984     51.687860   \n",
      "min    ...    -97.290000    -97.300000    -97.330000    -97.320000   \n",
      "50%    ...     -0.330000     -0.390000     -0.410000     -0.530000   \n",
      "max    ...   1009.660000   1017.000000   1148.680000   1128.480000   \n",
      "\n",
      "         growth_25d    growth_26d    growth_27d    growth_28d    growth_29d  \\\n",
      "count  21617.000000  21457.000000  21299.000000  21142.000000  20985.000000   \n",
      "mean       0.049294      0.027725     -0.014694     -0.086077     -0.163265   \n",
      "std       52.973627     54.749762     56.456057     58.503707     60.916532   \n",
      "min      -97.350000    -97.520000    -97.580000    -97.680000    -97.860000   \n",
      "50%       -0.570000     -0.780000     -0.900000     -1.070000     -1.230000   \n",
      "max     1166.100000   1402.520000   1378.210000   1865.530000   2191.880000   \n",
      "\n",
      "         growth_30d  \n",
      "count  20829.000000  \n",
      "mean      -0.227935  \n",
      "std       63.464207  \n",
      "min      -97.810000  \n",
      "50%       -1.390000  \n",
      "max     2619.850000  \n",
      "\n",
      "[6 rows x 30 columns]\n",
      "\n",
      "Recommendation:\n",
      "While the strategy focuses on the top 25% performers (75th percentile growth),\n",
      "the overall mean and median growth across all investments are negative for most holding periods.\n",
      "This suggests the strategy is risky and may not be profitable in general.\n",
      "It's generally not recommended to chase 'lucky' investments based on short-term growth.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "from requests import get  # Import get function directly from requests\n",
    "\n",
    "def get_and_analyze_ipos(start_year, end_year):\n",
    "  \"\"\"\n",
    "  Fetches IPO data for a given year range, retrieves prices, calculates growth rates, \n",
    "  and finds the optimal holding period. Analyzes additional statistics.\n",
    "\n",
    "  Args:\n",
    "      start_year (int): Starting year for IPO data retrieval.\n",
    "      end_year (int): Ending year for IPO data retrieval.\n",
    "\n",
    "  Returns:\n",
    "      int: The optimal holding period (number of days) with the highest 75th percentile growth.\n",
    "  \"\"\"\n",
    "  # Fetch and combine IPO data\n",
    "  ipos = pd.DataFrame()\n",
    "  for year in range(start_year, end_year + 1):\n",
    "    url = f\"https://stockanalysis.com/ipos/{year}/\"\n",
    "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "    response = get(url, headers=headers)\n",
    "    ipos = pd.concat([ipos, pd.read_html(response.text)[0]], ignore_index=True)\n",
    "  ipos['IPO Date'] = pd.to_datetime(ipos['IPO Date'])\n",
    "\n",
    "  # Filter IPOs and define adjustments (optional)\n",
    "  filtered_ipo = ipos[(ipos['IPO Date'] < '2024-03-01') & (ipos['Symbol'] != 'RYZB')]\n",
    "  ticker_adjustments = {\"PTHR\": \"PTHRF\", \"IBAC\": \"IBACU\"}  # Optional adjustments\n",
    "\n",
    "  # Download prices and calculate growth rates\n",
    "  price_data = {}\n",
    "  for symbol in filtered_ipo['Symbol']:\n",
    "    adjusted_symbol = ticker_adjustments.get(symbol, symbol)\n",
    "    try:\n",
    "      df = yf.download(adjusted_symbol, start=\"2023-01-01\", end=\"2024-03-01\")\n",
    "      if df is not None:\n",
    "        for i in range(1, 31):\n",
    "          df[f'growth_{i}d'] = ((df['Adj Close'].shift(-i) / df['Adj Close'] - 1) * 100).round(2)\n",
    "        price_data[symbol] = df\n",
    "    except Exception as e:\n",
    "      print(f\"Error downloading {symbol}: {e}\")\n",
    "\n",
    "  # Combine price data (if any) and calculate growth percentiles\n",
    "  results = pd.DataFrame()\n",
    "  for symbol, df in price_data.items():\n",
    "    results = pd.concat([results, df[[f'growth_{i}d' for i in range(1, 31)]]])\n",
    "  growth_quantile = results.quantile(0.75)\n",
    "\n",
    "  # Find optimal holding period with highest 75th percentile growth\n",
    "  optimal_days = growth_quantile.idxmax()\n",
    "\n",
    "  # Analyze additional statistics (mean, median)\n",
    "  growth_describe = results.describe(percentiles=[.5])  # Calculate mean, median\n",
    "\n",
    "  # Print results\n",
    "  print(f\"The optimal holding period is {optimal_days} days.\")\n",
    "  print(f\"Growth statistics (across all investments for each holding period):\")\n",
    "  print(growth_describe)\n",
    "\n",
    "  # Recommendation (Avoid pursuing for optimal X)\n",
    "  print(\"\\nRecommendation:\")\n",
    "  print(\"While the strategy focuses on the top 25% performers (75th percentile growth),\")\n",
    "  print(\"the overall mean and median growth across all investments are negative for most holding periods.\")\n",
    "  print(\"This suggests the strategy is risky and may not be profitable in general.\")\n",
    "  print(\"It's generally not recommended to chase 'lucky' investments based on short-term growth.\")\n",
    "\n",
    "  return optimal_days\n",
    "\n",
    "# Run the analysis and print results\n",
    "optimal_period = get_and_analyze_ipos(2023, 2024)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce07c8c-88d2-4854-ac22-35f25a00032d",
   "metadata": {},
   "source": [
    "Question 3: Is Growth Concentrated in the Largest Stocks?\n",
    "\n",
    "Get the share of days (percentage as int) when Large Stocks outperform (growth_7d - growth over 7 periods back) the Largest stocks?\n",
    "\n",
    "Reuse [Code Snippet 5] to obtain OHLCV stats for 33 stocks for 10 full years of data (2014-01-01 to 2023-12-31). You'll need to download slightly more data (7 periods before 2014-01-01 to calculate the growth_7d for the first 6 days correctly):\n",
    "\n",
    "US_STOCKS = ['MSFT', 'AAPL', 'GOOG', 'NVDA', 'AMZN', 'META', 'BRK-B', 'LLY', 'AVGO','V', 'JPM']\n",
    "\n",
    "EU_STOCKS = ['NVO','MC.PA', 'ASML', 'RMS.PA', 'OR.PA', 'SAP', 'ACN', 'TTE', 'SIE.DE','IDEXY','CDI.PA']\n",
    "\n",
    "INDIA_STOCKS = ['RELIANCE.NS','TCS.NS','HDB','BHARTIARTL.NS','IBN','SBIN.NS','LICI.NS','INFY','ITC.NS','HINDUNILVR.NS','LT.NS']\n",
    "\n",
    "LARGEST_STOCKS = US_STOCKS + EU_STOCKS + INDIA_STOCKS\n",
    "\n",
    "Now let's add the top 12-22 stocks (as of end-April 2024):\n",
    "\n",
    "NEW_US = ['TSLA','WMT','XOM','UNH','MA','PG','JNJ','MRK','HD','COST','ORCL']\n",
    "\n",
    "NEW_EU = ['PRX.AS','CDI.PA','AIR.PA','SU.PA','ETN','SNY','BUD','DTE.DE','ALV.DE','MDT','AI.PA','EL.PA']\n",
    "\n",
    "NEW_INDIA = ['BAJFINANCE.NS','MARUTI.NS','HCLTECH.NS','TATAMOTORS.NS','SUNPHARMA.NS','ONGC.NS','ADANIENT.NS','ADANIENT.NS','NTPC.NS','KOTAKBANK.NS','TITAN.NS']\n",
    "\n",
    "LARGE_STOCKS = NEW_EU + NEW_US + NEW_INDIA\n",
    "\n",
    "You should be able to obtain stats for 33 LARGEST STOCKS and 32 LARGE STOCKS (from the actual stats on Yahoo Finance)\n",
    "\n",
    "Calculate growth_7d for every stock and every day. Get the average daily growth_7d for the LARGEST_STOCKS group vs. the LARGE_STOCKS group.\n",
    "\n",
    "For example, for the first of data you should have:\n",
    "Date \tticker_category \tgrowth_7d\n",
    "2014-01-01 \tLARGE \t1.011684\n",
    "2014-01-01 \tLARGEST \t1.011797\n",
    "\n",
    "On that day, the LARGEST group was growing faster than LARGE one (new stocks).\n",
    "\n",
    "Calculate the number of days when the LARGE GROUP (new smaller stocks) outperforms the LARGEST GROUP, divide it by the total number of trading days (which should be 2595 days), and convert it to a percentage (closest INTEGER value). For example, if you find that 1700 out of 2595 days meet this condition, it means that 1700/2595 = 0.655, or approximately 66% of days, the LARGE stocks were growing faster than the LARGEST ones. This suggests that you should consider extending your dataset with more stocks to seek higher growth.\n",
    "\n",
    "HINT: you can use pandas.pivot_table() to \"flatten\" the table (LARGE and LARGEST growth_7d as columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "95e6076c-4904-4eea-8a8f-c9465f42281c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Share of days when LARGE STOCKS outperform LARGEST STOCKS: 47%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yfinance as yf\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Define the lists of tickers\n",
    "US_STOCKS = ['MSFT', 'AAPL', 'GOOG', 'NVDA', 'AMZN', 'META', 'BRK-B', 'LLY', 'AVGO', 'V', 'JPM']\n",
    "EU_STOCKS = ['NVO', 'MC.PA', 'ASML', 'RMS.PA', 'OR.PA', 'SAP', 'ACN', 'TTE', 'SIE.DE', 'IDEXY', 'CDI.PA']\n",
    "INDIA_STOCKS = ['RELIANCE.NS', 'TCS.NS', 'HDB', 'BHARTIARTL.NS', 'IBN', 'SBIN.NS', 'LICI.NS', 'INFY', 'ITC.NS', 'HINDUNILVR.NS', 'LT.NS']\n",
    "LARGEST_STOCKS = US_STOCKS + EU_STOCKS + INDIA_STOCKS\n",
    "\n",
    "NEW_US = ['TSLA', 'WMT', 'XOM', 'UNH', 'MA', 'PG', 'JNJ', 'MRK', 'HD', 'COST', 'ORCL']\n",
    "NEW_EU = ['PRX.AS', 'CDI.PA', 'AIR.PA', 'SU.PA', 'ETN', 'SNY', 'BUD', 'DTE.DE', 'ALV.DE', 'MDT', 'AI.PA', 'EL.PA']\n",
    "NEW_INDIA = ['BAJFINANCE.NS', 'MARUTI.NS', 'HCLTECH.NS', 'TATAMOTORS.NS', 'SUNPHARMA.NS', 'ONGC.NS', 'ADANIENT.NS', 'ADANIENT.NS', 'NTPC.NS', 'KOTAKBANK.NS', 'TITAN.NS']\n",
    "LARGE_STOCKS = NEW_US + NEW_EU + NEW_INDIA\n",
    "\n",
    "# Function to fetch OHLCV data for a ticker\n",
    "def fetch_prices(ticker, start_date, end_date):\n",
    "    try:\n",
    "        prices = yf.download(ticker, start=start_date, end=end_date)\n",
    "        return prices\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching data for {ticker}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Fetch OHLCV data for LARGEST_STOCKS and LARGE_STOCKS\n",
    "start_date = '2013-12-24'  # 7 periods before 2014-01-01\n",
    "end_date = '2023-12-31'\n",
    "all_tickers = LARGEST_STOCKS + LARGE_STOCKS\n",
    "\n",
    "prices = {}\n",
    "for ticker in all_tickers:\n",
    "    prices[ticker] = fetch_prices(ticker, start_date, end_date)\n",
    "\n",
    "# Calculate growth_7d for each stock and each day\n",
    "for ticker, df in prices.items():\n",
    "    df['growth_7d'] = df['Adj Close'] / df['Adj Close'].shift(7)\n",
    "\n",
    "# Flatten the table and calculate average daily growth_7d for LARGEST_STOCKS and LARGE_STOCKS\n",
    "flattened_prices = pd.concat(prices.values(), keys=prices.keys(), names=['Ticker'])\n",
    "pivot_table = flattened_prices.pivot_table(index='Date', columns='Ticker', values='growth_7d')\n",
    "\n",
    "average_growth_largest = pivot_table[LARGEST_STOCKS].mean(axis=1)\n",
    "average_growth_large = pivot_table[LARGE_STOCKS].mean(axis=1)\n",
    "\n",
    "# Calculate the number of days when LARGE GROUP outperforms LARGEST GROUP\n",
    "outperform_days = (average_growth_large > average_growth_largest).sum()\n",
    "\n",
    "# Calculate the share of outperforming days as a percentage\n",
    "total_days = len(average_growth_largest)\n",
    "share_outperform_days = int((outperform_days / total_days) * 100)\n",
    "\n",
    "print(f\"Share of days when LARGE STOCKS outperform LARGEST STOCKS: {share_outperform_days}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6907a051-88c3-44df-9b7e-415e84f08cf4",
   "metadata": {},
   "source": [
    "Question 4: Trying Another Technical Indicators strategy\n",
    "\n",
    "What's the total gross profit (in THOUSANDS of $) you'll get from trading on CCI (no fees assumption)?\n",
    "\n",
    "First, run the entire Colab to obtain the full DataFrame of data (after [Code Snippet 9]), and truncate it to the last full 10 years of data (2014-01-01 to 2023-12-31). If you encounter any difficulties running the Colab - you can download it using this link.\n",
    "\n",
    "Let's assume you've learned about the awesome CCI indicator (Commodity Channel Index), and decided to use only it for your operations.\n",
    "\n",
    "You defined the \"defensive\" value of a high threshould of 200, and you trade only on Fridays (Date.dt.dayofweek()==4).\n",
    "\n",
    "That is, every time you see that CCI is >200 for any stock (out of those 33), you'll invest $1000 (each record when CCI>200) at Adj.Close price and hold it for 1 week (5 trading days) in order to sell at the Adj. Close price.\n",
    "\n",
    "What's the expected gross profit (no fees) that you get in THOUSANDS $ (closest integer value) over many operations in 10 years? One operation calculations: if you invested $1000 and received $1010 in 5 days - you add $10 to gross profit, if you received $980 - add -$20 to gross profit. You need to sum these results over all trades (460 times in 10 years).\n",
    "\n",
    "Additional:\n",
    "\n",
    "    Add an approximate fees calculation over the 460 trades from this calculator https://www.degiro.ie/fees/calculator (Product:\"Shares, USA and Canada;\" Amount per transaction: \"1000 EUR\"; Transactions per year: \"460\")\n",
    "    are you still profitable on those trades?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "269e412f-2e7f-4668-978f-21509416cac7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Open         High          Low        Close  Adj Close_x  \\\n",
      "7011    37.349998    37.400002    37.099998    37.160000    31.233059   \n",
      "7012    37.200001    37.220001    36.599998    36.910000    31.022930   \n",
      "7013    36.849998    36.889999    36.110001    36.130001    30.367352   \n",
      "7014    36.330002    36.490002    36.209999    36.410000    30.602673   \n",
      "7015    36.000000    36.139999    35.580002    35.759998    30.056356   \n",
      "...           ...          ...          ...          ...          ...   \n",
      "5338  3424.000000  3496.000000  3408.600098  3477.949951  3477.949951   \n",
      "5339  3477.949951  3508.350098  3477.949951  3490.050049  3490.050049   \n",
      "5340  3510.000000  3549.000000  3504.149902  3544.000000  3544.000000   \n",
      "5341  3545.000000  3559.949951  3500.500000  3518.050049  3518.050049   \n",
      "5342  3531.000000  3540.000000  3495.000000  3526.000000  3526.000000   \n",
      "\n",
      "          Volume Ticker  Year      Month  Weekday  ... growth_brent_oil_7d  \\\n",
      "7011  30632200.0   MSFT  2014 2014-01-01        3  ...            0.964302   \n",
      "7012  31134800.0   MSFT  2014 2014-01-01        4  ...            0.958139   \n",
      "7013  43603700.0   MSFT  2014 2014-01-01        0  ...            0.953798   \n",
      "7014  35802800.0   MSFT  2014 2014-01-01        1  ...            0.958653   \n",
      "7015  59971700.0   MSFT  2014 2014-01-01        2  ...            0.955161   \n",
      "...          ...    ...   ...        ...      ...  ...                 ...   \n",
      "5338   1681707.0  LT.NS  2023 2023-12-01        4  ...            1.064772   \n",
      "5339   1072263.0  LT.NS  2023 2023-12-01        1  ...            1.058217   \n",
      "5340   1389266.0  LT.NS  2023 2023-12-01        2  ...            1.040496   \n",
      "5341   3371121.0  LT.NS  2023 2023-12-01        3  ...            1.005645   \n",
      "5342    968577.0  LT.NS  2023 2023-12-01        4  ...            0.972359   \n",
      "\n",
      "      growth_brent_oil_30d  growth_brent_oil_90d  growth_brent_oil_365d  \\\n",
      "7011              0.992998              0.970030               1.158676   \n",
      "7012              0.984707              0.961500               1.143209   \n",
      "7013              0.998223              0.968951               1.168236   \n",
      "7014              0.993430              0.977598               1.097648   \n",
      "7015              0.973383              0.974977               1.100781   \n",
      "...                    ...                   ...                    ...   \n",
      "5338              0.971018              0.939967               0.797881   \n",
      "5339              0.982429              0.956014               0.801404   \n",
      "5340              0.965806              0.943050               0.749506   \n",
      "5341              0.965632              0.932881               0.730228   \n",
      "5342              0.995092              0.925850               0.720539   \n",
      "\n",
      "      growth_btc_usd_1d  growth_btc_usd_3d  growth_btc_usd_7d  \\\n",
      "7011                NaN                NaN                NaN   \n",
      "7012                NaN                NaN                NaN   \n",
      "7013                NaN                NaN                NaN   \n",
      "7014                NaN                NaN                NaN   \n",
      "7015                NaN                NaN                NaN   \n",
      "...                 ...                ...                ...   \n",
      "5338           1.002935           1.040865           1.049324   \n",
      "5339           0.974945           0.972127           1.005911   \n",
      "5340           1.021694           1.009920           0.995203   \n",
      "5341           0.981240           0.977409           0.971705   \n",
      "5342           0.987603           0.990099           0.956850   \n",
      "\n",
      "      growth_btc_usd_30d  growth_btc_usd_90d  growth_btc_usd_365d  \n",
      "7011                 NaN                 NaN                  NaN  \n",
      "7012                 NaN                 NaN                  NaN  \n",
      "7013                 NaN                 NaN                  NaN  \n",
      "7014                 NaN                 NaN                  NaN  \n",
      "7015                 NaN                 NaN                  NaN  \n",
      "...                  ...                 ...                  ...  \n",
      "5338            1.175398            1.655339             2.614201  \n",
      "5339            1.134509            1.613511             2.513055  \n",
      "5340            1.166121            1.607712             2.598696  \n",
      "5341            1.126794            1.583988             2.575301  \n",
      "5342            1.112020            1.561092             2.529656  \n",
      "\n",
      "[80762 rows x 202 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Specify the path to your Parquet file\n",
    "file_path = \"C:/Users/msofy/Downloads/stock-markets-analytics-zoomcamp-2024/02-dataframe-analysis/stocks_df_combined_trunc_2014_2023.parquet.brotli\"\n",
    "\n",
    "# Load the Parquet file into a DataFrame\n",
    "data = pd.read_parquet(file_path)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e3778642-4f84-4ca7-bcab-4c25f038e5ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected gross profit (in THOUSANDS of $): 265\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the DataFrame containing the trading data\n",
    "df = pd.read_parquet(\"C:/Users/msofy/Downloads/stock-markets-analytics-zoomcamp-2024/02-dataframe-analysis/stocks_df_combined_trunc_2014_2023.parquet.brotli\")\n",
    "\n",
    "# Filter data to include only the last full 10 years\n",
    "start_date = '2014-01-01'\n",
    "end_date = '2023-12-31'\n",
    "data = data[(data['Date'] >= start_date) & (data['Date'] <= end_date)]\n",
    "\n",
    "# Filter data to include only Fridays\n",
    "friday_data = data[data['Date'].dt.dayofweek == 4]\n",
    "\n",
    "# Filter data to include only records where CCI > 200\n",
    "cci_threshold = 200\n",
    "cci_data = friday_data[friday_data['cci'] > cci_threshold]\n",
    "\n",
    "# Initialize gross profit\n",
    "gross_profit = 0\n",
    "\n",
    "# Calculate profit or loss for each trade\n",
    "for index, row in cci_data.iterrows():\n",
    "    ticker = row['Ticker']\n",
    "    price_invested = row['Adj Close_x']\n",
    "    \n",
    "    # Find the price after 5 trading days\n",
    "    sell_date = row['Date'] + timedelta(days=5)\n",
    "    sell_price_data = data[(data['Date'] >= sell_date) & (data['Date'] <= sell_date + timedelta(days=1)) & (data['Ticker'] == ticker)]\n",
    "    if not sell_price_data.empty:\n",
    "        price_sold = sell_price_data.iloc[0]['Adj Close_x']\n",
    "        profit_loss = (price_sold - price_invested) * 1000  # Assuming $1000 invested\n",
    "        gross_profit += profit_loss\n",
    "\n",
    "# Convert gross profit to thousands of dollars (rounded to the nearest integer)\n",
    "gross_profit_thousands = round(gross_profit / 1000)\n",
    "\n",
    "print(\"Expected gross profit (in THOUSANDS of $):\", gross_profit_thousands)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1017cc6e-681b-4f81-8b5a-322f901e6aeb",
   "metadata": {},
   "source": [
    "[EXPLORATORY] Question 5: Finding Your Strategy for IPOs\n",
    "\n",
    "You've seen in the first questions that the median and average investments are negative in IPOs, and you can't blindly invest in all deals.\n",
    "\n",
    "How would you correct/refine the approach? Briefly describe the steps and the data you'll try to get (it should be generally feasible to do it from public sources - no access to internal data of companies)?\n",
    "\n",
    "E.g. (some ideas) Do you want to focus on the specific vertical? Do you want to build a smart comparison vs. existing stocks on the market? Or you just will want to get some features (which features?) like total number of people in a company to find a segment of \"successful\" IPOs?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
